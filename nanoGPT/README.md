This project is a simple transformer built from scratch, following Andrej Karpathy's nanoGPT tutorial.

# nanoGPT - A Minimalist LLM for Local Execution

This project is a minimalist, character-level language model inspired by Andrej Karpathy's "nanoGPT". It is designed to be trained from scratch and run inference on a modern Apple Silicon laptop.

## Project Structure

- `train.py`: The script for training the model. It downloads the training data, preprocesses it, trains the model, and saves the weights.
- `chat.py`: The script for interacting with the trained model in a chatbot-style interface.
- `requirements.txt`: A list of the Python dependencies for this project.
- `input.txt`: The training data (Tiny Shakespeare). This is downloaded automatically by `train.py` on its first run.
- `model.pth`: The saved weights of the trained model. This is generated by `train.py` after training is complete.

## Technical Details

### Model Architecture

The model is a decoder-only GPT-2-style Transformer. It is designed to be lightweight and consists of the following key components and hyperparameters:
- **`n_layer`**: 6 transformer blocks
- **`n_head`**: 6 attention heads
- **`n_embd`**: 384 embedding dimensions
- **`block_size`**: 256 (the context length for predictions)
- **`dropout`**: 0.2 (to prevent overfitting)
- **`vocab_size`**: 65 (derived from the unique characters in `input.txt`)

The total number of parameters is approximately 10.8 million.

### Tokenization

The model uses a simple but effective **character-level tokenizer**. 

- The "vocabulary" is created by finding every unique character present in the `input.txt` training data.
- Each unique character is then assigned a unique integer index.
- Tokenization is the process of converting a string of text into the corresponding sequence of integers that the model can process.
- This approach is very simple and requires no external tokenizer libraries.

### Optimizations

To ensure efficient training on modern hardware, the following optimizations have been implemented in `train.py`:

- **Hardware Acceleration**: The script automatically detects and utilizes the Apple Silicon GPU (`mps` device) if it is available, falling back to the CPU otherwise. This provides a massive performance increase for training.
- **Automatic Mixed Precision (AMP)**: The training loop uses `torch.amp.autocast` to perform the majority of computations in half-precision (`float16`) instead of the standard full-precision (`float32`). This speeds up matrix calculations and reduces memory usage on the GPU.

## How to Run

**1. Navigate to the Project Directory:**

```bash
cd /Users/adityapande/LLM-study/nanoGPT
```

**2. Create and Activate a Virtual Environment:**

It is highly recommended to use a virtual environment to manage dependencies and avoid conflicts with system-wide packages.

```bash
# Create a virtual environment named .venv
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate
```
*Note: When you are finished, you can deactivate the environment by simply running the `deactivate` command.*

**3. Install Dependencies:**

Once the virtual environment is active, install the required Python libraries using `pip3`.

```bash
pip3 install -r requirements.txt
```

**4. Train the Model:**

Run the training script. This will first download the dataset and then begin training. This process may take a while. Once finished, it will save the trained model to `model.pth`.

```bash
python3 train.py
```

**5. Chat with the Model:**

After training is complete and `model.pth` has been created, you can start the interactive chat.

```bash
python3 chat.py
```

You will be greeted with a `>` prompt. Type your starting text and press Enter to see the model generate a response. To end the session, type `exit`.

## Sample Run

Below is a sample of the console output during the first 1500 steps of training. It shows the loss decreasing and the generated text samples gradually improving from random noise to something that resembles structured language.

```
(.venv) âžœ  nanoGPT python3 train.py
Using device: mps
10.788929 M parameters
/Users/adityapande/LLM-study/nanoGPT/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
step 0: train loss 4.2849, val loss 4.2823

OT;c'QByUeyuqKHuHfpOhNAf&V&PadR3n,iRqL'jqIazmJsxFiapkcbjliIrvsCp;ddxmFHKMng?uUN
?L,I;ekHPC3aOc::MQdB
---
step 500: train loss 2.7762, val loss 2.7830

Nodm er n.

Ma--
Cl,
OLUCLIO asagneetkuefowapunueW:
Toohot'ds, t kltww gho  oo dalthu 'l  egsidu res
---
step 1000: train loss 2.6718, val loss 2.7210

Serysha yseoed k oytl os.

JAPULET:
Giv ao ntdie wl bucrw pdadb,
Gawsafole ct, iyn t  teerlttotHaam
---
step 1500: train loss 2.6373, val loss 2.6943

Asoydid, nmh ene'denumtlede-wleaishiasara;
I ae, kysete i rtt eaeat hahsaoi boe:
Tofo fytita e wdd p
---
```